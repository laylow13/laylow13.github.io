[{"title":"Llama入门","url":"/2025/12/01/llama/","content":"\n### 导入与模型配置\nLLaMA 的基本超参数可表示为向量 $\\theta = \\{V, d, L, H, H_{\\text{kv}}, d_{\\text{ff}}\\}$，分别对应词表大小、隐层维度、层数、注意力头数、键值头数、前馈层宽度。下面的代码加载必要库并定义这些超参数的数据类。\n\n```python\nimport math\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n@dataclass\nclass LLaMAConfig:\n    vocab_size: int = 32000\n    dim: int = 4096\n    n_layers: int = 24\n    n_heads: int = 32\n    n_kv_heads: int = 8\n    rope_theta: float = 10000.0\n    max_seq_len: int = 4096\n    intermediate_size: int = 11008\n    dropout: float = 0.0\n```\n\n### RMSNorm\nRMSNorm 在每个 token 向量维度上进行均方根归一化：\n$$\\mathrm{RMSNorm}(x)=\\gamma \\cdot \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^{d} x_i^2 + \\epsilon}}$$\n可在深层 Transformer 中保持数值稳定性，避免 LayerNorm 的均值项带来的额外计算。\n\n```python\nclass RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-6):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(dim))\n        self.eps = eps\n\n    def forward(self, x):\n        norm = x.pow(2).mean(-1, keepdim=True)\n        x = x * torch.rsqrt(norm + self.eps)\n        return self.weight * x\n```\n\n### 旋转位置编码（RoPE）\nRoPE 将二维正余弦旋转直接施加在注意力空间：\n$$\\operatorname{RoPE}(x_{2i}, x_{2i+1}) =\n\\begin{bmatrix}\n x_{2i} & -x_{2i+1} \\\\\n x_{2i+1} & x_{2i}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\cos(\\theta_i) \\\\\n\\sin(\\theta_i)\n\\end{bmatrix}$$\n使得注意力得分隐式编码相对位置信息，满足 $\\langle \\operatorname{RoPE}(q_k), \\operatorname{RoPE}(k_j)\\rangle = \\langle q_k, R_{k-j} k_j \\rangle$。\n\n```python\nclass RotaryEmbedding(nn.Module):\n    def __init__(self, dim, theta=10000.0, max_seq_len=4096):\n        super().__init__()\n        inv_freq = torch.exp(-math.log(theta) * (torch.arange(0, dim, 2).float() / dim))\n        t = torch.arange(max_seq_len, dtype=torch.float32)\n        freqs = inv_freq.unsqueeze(0) * t.unsqueeze(1)\n        self.register_buffer(\"cos\", freqs.cos()[None, None, :, :], persistent=False)\n        self.register_buffer(\"sin\", freqs.sin()[None, None, :, :], persistent=False)\n\n    def forward(self, q, k, seq_len):\n        cos = self.cos[..., :seq_len, :]\n        sin = self.sin[..., :seq_len, :]\n\n        def apply_rotary(x):\n            half = x.shape[-1] // 2\n            x1, x2 = x[..., :half], x[..., half:]\n            return torch.cat((x1*cos - x2*sin, x1*sin + x2*cos), dim=-1)\n\n        return apply_rotary(q), apply_rotary(k)\n```\n\n### SwiGLU 前馈网络\nSwiGLU 组合了 SiLU 门控：\n$$\\text{SwiGLU}(x)=W_3\\Big(\\text{SiLU}(W_1 x) \\odot W_2 x\\Big)$$\n其中 $\\text{SiLU}(z)=z\\cdot\\sigma(z)$，可提升前馈层表达力并保持梯度稳定。\n\n```python\nclass SwiGLU(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w3 = nn.Linear(hidden_dim, dim, bias=False)\n\n    def forward(self, x):\n        return self.w3(F.silu(self.w1(x)) * self.w2(x))\n```\n\n### 组查询注意力（GQA）\nGQA 通过共享键值头降低内存：\n$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_h}} + M\\right)V$$\n其中 $K,V$ 以 $H_{\\text{kv}}$ 头计算，然后复制到 $H$ 个查询头，$M$ 为因果掩码。RoPE 在 $Q,K$ 上注入位置依赖。\n\n```python\nclass GQAttention(nn.Module):\n    def __init__(self, dim, n_heads, n_kv_heads, rope):\n        super().__init__()\n        self.n_heads = n_heads\n        self.n_kv = n_kv_heads\n        self.head_dim = dim // n_heads\n        self.rope = rope\n\n        self.q_proj = nn.Linear(dim, dim, bias=False)\n        self.k_proj = nn.Linear(dim, self.head_dim * n_kv_heads, bias=False)\n        self.v_proj = nn.Linear(dim, self.head_dim * n_kv_heads, bias=False)\n        self.o_proj = nn.Linear(dim, dim, bias=False)\n\n    def forward(self, x, mask=None, cache=None):\n        B, T, C = x.shape\n        q = self.q_proj(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n        k = self.k_proj(x).view(B, T, self.n_kv, self.head_dim).transpose(1, 2)\n        v = self.v_proj(x).view(B, T, self.n_kv, self.head_dim).transpose(1, 2)\n\n        q, k = self.rope(q, k, T)\n        if cache is not None:\n            k = torch.cat([cache[\"k\"], k], dim=2)\n            v = torch.cat([cache[\"v\"], v], dim=2)\n            T = k.shape[2]\n        k = k.repeat_interleave(self.n_heads // self.n_kv, dim=1)\n        v = v.repeat_interleave(self.n_heads // self.n_kv, dim=1)\n\n        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        if mask is not None:\n            attn = attn + mask\n        attn = attn.softmax(dim=-1)\n        out = torch.matmul(attn, v).transpose(1, 2).contiguous().view(B, T, C)\n        return self.o_proj(out), {\"k\": k, \"v\": v}\n```\n\n### LLaMA Block 结构\n每个 Block 采用 pre-norm 残差结构：\n$$x' = x + \\text{Attention}(\\text{RMSNorm}(x))$$\n$$x'' = x' + \\text{FFN}(\\text{RMSNorm}(x'))$$\n确保梯度在深层网络中稳定传播。\n\n```python\nclass LLaMABlock(nn.Module):\n    def __init__(self, cfg, rope):\n        super().__init__()\n        self.attn_norm = RMSNorm(cfg.dim)\n        self.ff_norm = RMSNorm(cfg.dim)\n        self.attn = GQAttention(cfg.dim, cfg.n_heads, cfg.n_kv_heads, rope)\n        self.ff = SwiGLU(cfg.dim, cfg.intermediate_size)\n\n    def forward(self, x, mask=None, cache=None):\n        attn_out, cache = self.attn(self.attn_norm(x), mask, cache)\n        x = x + attn_out\n        x = x + self.ff(self.ff_norm(x))\n        return x, cache\n```\n\n### LLaMA 模型整体\n模型包含词嵌入、$L$ 个 Block 以及输出头：\n$$h_0 = E(x),\\quad h_{l+1} = \\text{Block}_l(h_l)$$\n$$\\text{Logits} = h_L W_{\\text{lm}}^\\top$$\n可配合缓存 $\\{K,V\\}$ 实现增量推理。\n\n```python\nclass LLaMAModel(nn.Module):\n    def __init__(self, cfg: LLaMAConfig):\n        super().__init__()\n        self.cfg = cfg\n        self.embed = nn.Embedding(cfg.vocab_size, cfg.dim)\n        rope = RotaryEmbedding(cfg.dim // cfg.n_heads, cfg.rope_theta, cfg.max_seq_len)\n        self.layers = nn.ModuleList([LLaMABlock(cfg, rope) for _ in range(cfg.n_layers)])\n        self.norm = RMSNorm(cfg.dim)\n        self.lm_head = nn.Linear(cfg.dim, cfg.vocab_size, bias=False)\n \n    def forward(self, input_ids, mask=None, cache=None):\n        x = self.embed(input_ids)\n        new_caches = []\n        for i, layer in enumerate(self.layers):\n            layer_cache = None if cache is None else cache[i]\n            x, layer_cache = layer(x, mask, layer_cache)\n            new_caches.append(layer_cache)\n        x = self.norm(x)\n        logits = self.lm_head(x)\n        return logits, new_caches\n```\n\n### 传统正余弦位置编码\nTransformer 原始位置编码通过固定正余弦模式：\n$$\\text{PE}_{(p,2i)} = \\sin\\left(\\frac{p}{10000^{2i/d}}\\right),\\qquad \\text{PE}_{(p,2i+1)} = \\cos\\left(\\frac{p}{10000^{2i/d}}\\right)$$\n并与词向量相加：$x_p' = x_p + \\text{PE}_p$，用于绝对位置信息注入。\n\n```python\nclass SinusoidalPositionalEncoding(nn.Module):\n    def __init__(self, dim, max_seq_len=4096):\n        super().__init__()\n        pe = torch.zeros(max_seq_len, dim)\n        position = torch.arange(0, max_seq_len, dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.register_buffer(\"pe\", pe.unsqueeze(0), persistent=False)\n\n    def forward(self, x, start_pos=0):\n        seq_len = x.size(1)\n        return x + self.pe[:, start_pos:start_pos + seq_len]\n```\n"},{"title":"Hello World","url":"/2025/12/01/hello-world/","content":"时隔一年，重启了这个blog。\n打算之后在这里记录一些RL, VLA，World Model相关的paper reading，主要关注autonomous driving和robotics。\n<!-- $$ Loss $$ -->"}]